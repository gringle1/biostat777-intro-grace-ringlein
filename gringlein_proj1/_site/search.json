[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Grace V. Ringlein",
    "section": "",
    "text": "Biostatistics PhD student and cat-mom of two."
  },
  {
    "objectID": "index.html#about-me",
    "href": "index.html#about-me",
    "title": "Grace V Ringlein",
    "section": "",
    "text": "I am a second year PhD Student in Biostatistics at the Johns Hopkins Bloomberg School of Public Health.\n\nWho are you? What makes you special?\nWhat do you stand for? For example, maybe you are passionate about open science and you prioritize publishing in open-access j ournals or publishing open source software. Who is your audience? This is important in terms of helping you understand your professional community as a scientist. What does the community value or not value? Does this align with your interests? What is your goal? This is a very personal question and there are no wrong answers. However, it is helpful to understand what your short-term and long-terms goals are. Some questions you might think about as you try to answer these two questions are: What motivates you? What projects have others complimented you on? Which projects can I spend hours on and not feel overwhelmed or drained? Fill in the following: “I want to be a leader in … &lt;&gt;” “In 5-10 years, I want to … &lt;&lt; insert thing you want to have accomplished &gt;&gt;”"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I am originally from Lancaster, PA and attended the University of Pennsylvania from 2016-2020, where I received my BA and MS in Physics and Astronomy. After graduating, I became a Postbaccalaureate research assistant in the Section on Development and Affective Neuroscience in the National Institute of Mental Health.\nIn 2022, I started my PhD in Biostatistics at the Johns Hopkins Bloomberg School of Public Health and I am currently working on projects related to causal inference, algorithmic fairness, climate change, and mental health."
  },
  {
    "objectID": "about.html#about-me",
    "href": "about.html#about-me",
    "title": "About",
    "section": "",
    "text": "I am originally from Lancaster, PA and attended the University of Pennsylvania from 2016-2020, where I received my BA and MS in Physics and Astronomy. After graduating, I became a Postbaccalaureate research assistant in the Section on Development and Affective Neuroscience in the National Institute of Mental Health.\nIn 2022, I started my PhD in Biostatistics at the Johns Hopkins Bloomberg School of Public Health and I am currently working on projects related to causal inference, algorithmic fairness, climate change, and mental health."
  },
  {
    "objectID": "example.html",
    "href": "example.html",
    "title": "Example Analysis",
    "section": "",
    "text": "This example analysis is intended to provide an example of how to conduct a descriptive analysis of multiple waves of data from the Pew Research Center American Trends Panel (ATP)1, a nationally representative survey.\n\n\nIn particular, we look at descriptive plots to investigate the frequency of financial worries before the start of the COVID-19 pandemic, in September 2019 and at the start of the pandemic in April 2020.\n\nHow did frequency of financial worry changed between 2019 and the start of the pandemic?\nWere there differences in how financial worry changed between those who did and did not report household job or income loss at the start of the pandemic?\nWere there differences in how financial worry changed between those who did and did not report household job or income loss at the start of the pandemic, for specfic types of financial worry (worry about debt, bills, health care, and retirement)?\n\n\n\n\nThe intended analysis is a researcher interested in using multiple waves of Pew Research Center American Trends Panel data."
  },
  {
    "objectID": "example.html#example-analysis-of-pew-research-center-american-trends-panel-data",
    "href": "example.html#example-analysis-of-pew-research-center-american-trends-panel-data",
    "title": "Example Analysis",
    "section": "",
    "text": "This example analysis is intended to provide an example of how to conduct a descriptive analysis of multiple waves of data from the Pew Research Center American Trends Panel (ATP)1, a nationally representative survey.\n\n\nIn particular, we look at descriptive plots to investigate the frequency of financial worries before the start of the COVID-19 pandemic, in September 2019 and at the start of the pandemic in April 2020.\n\nHow did frequency of financial worry changed between 2019 and the start of the pandemic?\nWere there differences in how financial worry changed between those who did and did not report household job or income loss at the start of the pandemic?\nWere there differences in how financial worry changed between those who did and did not report household job or income loss at the start of the pandemic, for specfic types of financial worry (worry about debt, bills, health care, and retirement)?\n\n\n\n\nThe intended analysis is a researcher interested in using multiple waves of Pew Research Center American Trends Panel data."
  },
  {
    "objectID": "example.html#set-up-analysis",
    "href": "example.html#set-up-analysis",
    "title": "Example Analysis",
    "section": "Set-up analysis",
    "text": "Set-up analysis\nAnalysis is conducted in R. We will use the following packages:\n\nlibrary(foreign)\nlibrary(tidyverse)\nlibrary(here)\nlibrary(dplyr)\n\nThe downloaded data files are placed into a folder called “data” in the working directory. The foreign package allows us to read in the data in a labelled format. We will use survey weights from Wave 654.\n\nW54 &lt;- foreign::read.spss(here(\"data\",\"ATP W54.sav\"), to.data.frame = TRUE)\nW65 &lt;- foreign::read.spss(here(\"data\",\"ATP W65.sav\"), to.data.frame = TRUE)\n\n\n#RENAME COLUMNS THAT DON'T HAVE WAVE NUM\nW54 &lt;- W54 %&gt;% rename_at(vars(!matches(\"*(_W54)\"),-QKEY),\n                         ~paste0(., '_W54'))\nW65 &lt;- W65 %&gt;% rename_at(vars(!matches(\"*(_W65)\"),-QKEY),\n                         ~paste0(., '_W65'))\n\nW54$RESPONDED_W54 &lt;- c(rep(1,nrow(W54)))\nW65$RESPONDED_W65 &lt;- c(rep(1,nrow(W65)))\n\n\nall &lt;- W54 %&gt;% \n  inner_join(W65,by=\"QKEY\") \n\n\nall &lt;- all %&gt;% mutate(\n  \"COVIDWORK_comp_W65\" = case_when(\n    is.na(COVIDWORK_a_W65) & is.na(COVIDWORK_b_W65) ~ \"Refused/Missing\",\n    COVIDWORK_a_W65 == \"Yes, has happened\" | COVIDWORK_b_W65 == \"Yes, has happened\" ~ \"Yes, has happened\",\n    TRUE ~ \"No, has not happened\"))\n\n\nall &lt;- all %&gt;% select(QKEY,RESPONDED_W54,RESPONDED_W65,WEIGHT_W65, income_loss=COVIDWORK_comp_W65,contains(\"WORRY\")) %&gt;% relocate(\"QKEY\",contains(\"RESPONDED\")) \n\nall %&gt;% select(contains(\"WORRY\")) %&gt;% head()\n\n  WORRY2a_W54      WORRY2b_W54      WORRY2c_W54      WORRY2d_W54\n1       Never        Sometimes           Rarely        Sometimes\n2   Sometimes Almost every day Almost every day Almost every day\n3       Never             &lt;NA&gt;            Never            Never\n4   Every day             &lt;NA&gt;        Every day        Every day\n5   Sometimes        Sometimes        Sometimes        Sometimes\n6      Rarely            Never            Never Almost every day\n       WORRY2e_W54 WORRY2_a_W65 WORRY2_b_W65 WORRY2_c_W65     WORRY2_d_W65\n1 Almost every day       Rarely       Rarely       Rarely           Rarely\n2 Almost every day    Sometimes         &lt;NA&gt;    Sometimes Almost every day\n3           Rarely        Never         &lt;NA&gt;        Never            Never\n4        Every day    Every day         &lt;NA&gt;    Every day        Every day\n5        Sometimes    Sometimes    Sometimes    Sometimes Almost every day\n6           Rarely        Never         &lt;NA&gt;        Never           Rarely\n      WORRY2_e_W65 WORRY2_f_W65\n1        Sometimes       Rarely\n2 Almost every day         &lt;NA&gt;\n3            Never         &lt;NA&gt;\n4        Every day         &lt;NA&gt;\n5 Almost every day       Rarely\n6        Sometimes         &lt;NA&gt;\n\nall &lt;- all %&gt;% rename_at(vars(matches(\"(WORRY)(.*)(W65)\")),~str_remove(.,\"_\"))\n\n\nlibrary(gtsummary)\ntbl_summary(all,by=\"income_loss\",include=contains(\"WORRY\"))\n\n\n\n\n\n  \n    \n    \n      Characteristic\n      No, has not happened, N = 1,6661\n      Yes, has happened, N = 1,0811\n    \n  \n  \n    WORRY2a_W54\n\n\n        Every day\n273 (16%)\n285 (26%)\n        Almost every day\n216 (13%)\n193 (18%)\n        Sometimes\n427 (26%)\n299 (28%)\n        Rarely\n330 (20%)\n149 (14%)\n        Never\n410 (25%)\n147 (14%)\n        Refused\n10 (0.6%)\n8 (0.7%)\n    WORRY2b_W54\n\n\n        Every day\n66 (7.1%)\n88 (11%)\n        Almost every day\n59 (6.3%)\n78 (9.9%)\n        Sometimes\n259 (28%)\n230 (29%)\n        Rarely\n293 (31%)\n239 (30%)\n        Never\n253 (27%)\n144 (18%)\n        Refused\n5 (0.5%)\n12 (1.5%)\n        Unknown\n731\n290\n    WORRY2c_W54\n\n\n        Every day\n232 (14%)\n260 (24%)\n        Almost every day\n222 (13%)\n188 (17%)\n        Sometimes\n473 (28%)\n322 (30%)\n        Rarely\n401 (24%)\n198 (18%)\n        Never\n332 (20%)\n108 (10.0%)\n        Refused\n6 (0.4%)\n5 (0.5%)\n    WORRY2d_W54\n\n\n        Every day\n233 (14%)\n246 (23%)\n        Almost every day\n236 (14%)\n208 (19%)\n        Sometimes\n643 (39%)\n377 (35%)\n        Rarely\n352 (21%)\n162 (15%)\n        Never\n193 (12%)\n79 (7.3%)\n        Refused\n9 (0.5%)\n9 (0.8%)\n    WORRY2e_W54\n\n\n        Every day\n292 (18%)\n303 (28%)\n        Almost every day\n294 (18%)\n235 (22%)\n        Sometimes\n565 (34%)\n346 (32%)\n        Rarely\n239 (14%)\n106 (9.8%)\n        Never\n241 (14%)\n78 (7.2%)\n        Refused\n35 (2.1%)\n13 (1.2%)\n    WORRY2a_W65\n\n\n        Every day\n196 (12%)\n245 (23%)\n        Almost every day\n206 (12%)\n208 (19%)\n        Sometimes\n497 (30%)\n322 (30%)\n        Rarely\n373 (22%)\n172 (16%)\n        Never\n390 (23%)\n132 (12%)\n        Refused\n4 (0.2%)\n2 (0.2%)\n    WORRY2b_W65\n\n\n        Every day\n52 (6.1%)\n99 (15%)\n        Almost every day\n63 (7.4%)\n69 (10%)\n        Sometimes\n280 (33%)\n233 (35%)\n        Rarely\n296 (35%)\n189 (28%)\n        Never\n155 (18%)\n76 (11%)\n        Refused\n2 (0.2%)\n4 (0.6%)\n        Unknown\n818\n411\n    WORRY2c_W65\n\n\n        Every day\n187 (11%)\n262 (24%)\n        Almost every day\n169 (10%)\n206 (19%)\n        Sometimes\n497 (30%)\n330 (31%)\n        Rarely\n496 (30%)\n195 (18%)\n        Never\n313 (19%)\n85 (7.9%)\n        Refused\n4 (0.2%)\n3 (0.3%)\n    WORRY2d_W65\n\n\n        Every day\n164 (9.8%)\n227 (21%)\n        Almost every day\n207 (12%)\n211 (20%)\n        Sometimes\n693 (42%)\n395 (37%)\n        Rarely\n385 (23%)\n175 (16%)\n        Never\n212 (13%)\n69 (6.4%)\n        Refused\n5 (0.3%)\n4 (0.4%)\n    WORRY2e_W65\n\n\n        Every day\n237 (14%)\n272 (25%)\n        Almost every day\n243 (15%)\n208 (19%)\n        Sometimes\n605 (36%)\n385 (36%)\n        Rarely\n294 (18%)\n139 (13%)\n        Never\n267 (16%)\n69 (6.4%)\n        Refused\n20 (1.2%)\n8 (0.7%)\n    WORRY2f_W65\n\n\n        Every day\n48 (5.7%)\n150 (22%)\n        Almost every day\n60 (7.1%)\n128 (19%)\n        Sometimes\n228 (27%)\n202 (30%)\n        Rarely\n275 (32%)\n120 (18%)\n        Never\n236 (28%)\n67 (10%)\n        Refused\n1 (0.1%)\n3 (0.4%)\n        Unknown\n818\n411\n  \n  \n  \n    \n      1 n (%)\n    \n  \n\n\n\n\n\nlong &lt;- all %&gt;% pivot_longer(cols = starts_with(\"WORRY\"),\n  names_to = c(\".value\",\"Wave\"),\n  names_pattern = \"(.*_)(W\\\\d+)\") %&gt;% \n  pivot_longer(cols = starts_with(\"WORRY\"),\n  names_to = c(\".value\",\"Q\"),\n  names_pattern = \"(.*)(\\\\d.)\") %&gt;% mutate(\"WORRY_int\" = case_when(\n    WORRY == \"Never\" ~ 0,\n    WORRY == \"Rarely\" ~ 1,\n    WORRY == \"Sometimes\" ~ 2,\n    WORRY == \"Almost every day\" ~ 3,\n    WORRY == \"Every day\" ~ 4,\n    TRUE ~ NA\n  ))\n\nlong %&gt;% filter(!(Q %in% c(\"2b\",\"2f\")))%&gt;% filter(is.na(WORRY_int))\n\n# A tibble: 145 × 9\n     QKEY RESPONDED_W54 RESPONDED_W65 WEIGHT_W65 income_loss   Wave  Q     WORRY\n    &lt;dbl&gt;         &lt;dbl&gt;         &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt;         &lt;chr&gt; &lt;chr&gt; &lt;fct&gt;\n 1 103956             1             1      0.712 No, has not … W65   2e    Refu…\n 2 133700             1             1      0.692 No, has not … W54   2e    Refu…\n 3 133700             1             1      0.692 No, has not … W65   2e    Refu…\n 4 176214             1             1      1.22  No, has not … W54   2e    Refu…\n 5 176214             1             1      1.22  No, has not … W65   2e    Refu…\n 6 194860             1             1      0.947 Yes, has hap… W65   2e    Refu…\n 7 196645             1             1      5.42  No, has not … W54   2a    Refu…\n 8 207573             1             1      0.912 No, has not … W54   2d    Refu…\n 9 225546             1             1      0.772 No, has not … W65   2e    Refu…\n10 227117             1             1      0.477 Yes, has hap… W54   2e    Refu…\n# ℹ 135 more rows\n# ℹ 1 more variable: WORRY_int &lt;dbl&gt;\n\nlong %&gt;% group_by(QKEY) %&gt;% summarize(\"perc_missing\"=sum(is.na(WORRY))/6) %&gt;% ggplot() + geom_histogram(aes(x=perc_missing))\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\nlong %&gt;% group_by(Q,Wave) %&gt;% summarize(\"n_miss\"=sum(is.na(WORRY_int)),\"perc_missing\"=n_miss/n())\n\n`summarise()` has grouped output by 'Q'. You can override using the `.groups`\nargument.\n\n\n# A tibble: 12 × 4\n# Groups:   Q [6]\n   Q     Wave  n_miss perc_missing\n   &lt;chr&gt; &lt;chr&gt;  &lt;int&gt;        &lt;dbl&gt;\n 1 2a    W54       18      0.00655\n 2 2a    W65        6      0.00218\n 3 2b    W54     1038      0.378  \n 4 2b    W65     1235      0.450  \n 5 2c    W54       11      0.00400\n 6 2c    W65        7      0.00255\n 7 2d    W54       18      0.00655\n 8 2d    W65        9      0.00328\n 9 2e    W54       48      0.0175 \n10 2e    W65       28      0.0102 \n11 2f    W54     2747      1      \n12 2f    W65     1233      0.449  \n\nlong &lt;- long %&gt;% filter(!(Q %in% c(\"2b\",\"2f\")))\nlong %&gt;% filter(!(Q %in% c(\"2b\",\"2f\"))) %&gt;% group_by(Wave,Q,income_loss) %&gt;% summarize(mean = mean(WORRY_int,na.rm=TRUE),se=sd(WORRY_int,na.rm=TRUE)/sqrt(n())) %&gt;% ggplot() + geom_point(aes(x=Wave, y=mean,colour=Q)) + geom_line(aes(x=Wave, y=mean,colour=Q,linetype=income_loss,group=interaction(income_loss,Q))) + geom_errorbar(aes(x=Wave,ymin=(mean-1.96*se),ymax=(mean + 1.96*se),colour=Q),width=.05)+ facet_wrap(.~Q)\n\n`summarise()` has grouped output by 'Wave', 'Q'. You can override using the\n`.groups` argument.\n\n\n\n\nlong %&gt;% filter(!(Q %in% c(\"2f\"))) %&gt;% ggplot() + geom_bar(aes(x=WORRY,fill=Wave),position=\"dodge\") + facet_grid(rows=vars(Q))\n\n\n\nby_year &lt;- long %&gt;% pivot_wider(names_from=Wave,values_from=c(WORRY,WORRY_int)) \n\nby_year %&gt;% filter(!(Q %in% c(\"2f\"))) %&gt;% mutate(change = case_when(\n  WORRY_int_W54 &gt; WORRY_int_W65 ~ \"Decreased\",\n   WORRY_int_W54 &lt; WORRY_int_W65 ~ \"Increased\",\n  TRUE ~ \"Same\")) %&gt;% \n  ggplot() + geom_bar(aes(x=income_loss,fill=change),position=\"fill\") + facet_wrap(.~Q)\n\n\n\nby_year %&gt;% filter(!(Q %in% c(\"2f\")),!is.na(WORRY_int_W54),!is.na(WORRY_int_W65)) %&gt;% group_by(Q,income_loss) %&gt;% summarize(sum_W54 = sum(WORRY_int_W54),sum_W65 = sum(WORRY_int_W65),change= case_when(\n  sum_W54 &gt; sum_W65 ~ \"Decreased\",\n   sum_W54 &lt; sum_W65 ~ \"Increased\",\n  TRUE ~ \"Same\")) %&gt;% \n  ggplot() + geom_bar(aes(x=income_loss,fill=change),position=\"fill\")\n\n`summarise()` has grouped output by 'Q'. You can override using the `.groups`\nargument.\n\n\n\n\nby_year %&gt;% filter(!(Q %in% c(\"2f\")),!is.na(WORRY_int_W54),!is.na(WORRY_int_W65)) %&gt;% group_by(Q,income_loss) %&gt;% summarize(sum_W54 = sum(WORRY_int_W54),sum_W65 = sum(WORRY_int_W65),change= case_when(\n  sum_W54 &gt; sum_W65 ~ \"Decreased\",\n   sum_W54 &lt; sum_W65 ~ \"Increased\",\n  TRUE ~ \"Same\")) %&gt;%\n  ggplot() + geom_bar(aes(x=income_loss,fill=change),position=\"fill\")\n\n`summarise()` has grouped output by 'Q'. You can override using the `.groups`\nargument.\n\n\n\n\n\nPlots should have titles, subtitles, captions, and human-understandable axis labels. At least one plot should using a type of faceting (facet_grid() or facet_wrap()). Your analysis must include one image or table (not one you created yourself, but one you have saved locally or one from the web).\nFor example, it could be to a website or paper from where the original data came from or it could be to a paper describing a method you are using to analyze the data. Your analysis must include the use of at least 1 margin content. You must summarize your analysis and/or results with a paragraph (4-6 sentences). At the end of the data analysis, list out each of the functions you used from each of the packages (dplyr, tidyr, and ggplot2) to help the TA with respect to making sure you met all the requirements described above."
  },
  {
    "objectID": "example.html#conclusion",
    "href": "example.html#conclusion",
    "title": "Example Analysis",
    "section": "Conclusion",
    "text": "Conclusion\nI used a\n\nNotes:\nData wrangling functions used:\n\nselect\ngroup_by\nsummarize\nmutate\nfilter\npivot_longer\nrename_at\n\n\n\nPlotting functions used:\n\ngeom_bar\ngeom_point\ngeom_errorbar\ngeom_line\nfacet_wrap\n\n\n\nReferences\n\n\n1. The American Trends Panel. https://www.pewresearch.org/our-methods/u-s-surveys/the-american-trends-panel/\n\n\n2. Bartik A, Bertrand M, Lin F, Rothstein J, Unrath M. Measuring the Labor Market at the Onset of the COVID-19 Crisis. National Bureau of Economic Research; 2020:w27613. doi:10.3386/w27613\n\n\n3. Wilkinson LR. Financial Strain and Mental Health Among Older Adults During the Great Recession. The Journals of Gerontology Series B: Psychological Sciences and Social Sciences. 2016;71(4):745-754. doi:10.1093/geronb/gbw001\n\n\n4. Pew Research Center’s American Trends Panel Wave 54 Methodology Report. Pew Research Center; 2019.\n\n\n5. Pew Research Center’s American Trends Panel Wave 65 Methodology Report. Pew Research Center; 2020."
  },
  {
    "objectID": "example.html#background",
    "href": "example.html#background",
    "title": "Example Analysis",
    "section": "Background",
    "text": "Background\nRecord levels of unemployment were reached at the start of the pandemic and were experienced unequally across socioeconomic groups2. Most analyses focus on objective measures of financial hardship, but perceived financial strain predictor of mental health status above and beyond objective financial status3. Thus, in this analysis, I investigate changes in worry about bills before and after the pandemic."
  },
  {
    "objectID": "example.html#motivation",
    "href": "example.html#motivation",
    "title": "Example Analysis",
    "section": "Motivation",
    "text": "Motivation\nRecord levels of unemployment were reached at the start of the pandemic and were experienced unequally across socioeconomic groups2. Most analyses focus on objective measures of financial hardship, but perceived financial strain predictor of mental health status above and beyond objective financial status3. Thus, in this analysis, I investigate changes in worry about bills before and after the pandemic."
  },
  {
    "objectID": "example.html#motivation-for-the-example",
    "href": "example.html#motivation-for-the-example",
    "title": "Example Analysis",
    "section": "Motivation for the example",
    "text": "Motivation for the example\nRecord levels of unemployment were reached at the start of the pandemic and were experienced unequally across socioeconomic groups2. Most analyses of the COVID-19 recession focus on objective measures of financial status, but perceived financial strain may be an important component of economic hardship, as it can be a predictor of mental health status above and beyond objective financial status3. Thus, I investigate changes in worry about bills before and after the pandemic.\n\n\n\nFigure 1. Unemployment during the pandemic by income group"
  },
  {
    "objectID": "example.html#data-download",
    "href": "example.html#data-download",
    "title": "Example Analysis",
    "section": "Data download",
    "text": "Data download\nThe ATP data is publicly available one year after data collection and can be downloaded here: https://www.pewresearch.org/american-trends-panel-datasets/\nFor this analysis, we will use Wave 544 and Wave 655.\n\n\n\n\n\n\nDownloaded zip vile\n\n\n\nThe download for each wave will contain a “.sav” file with data and additional pdf files containing information about the data including descriptions of variables in the data."
  },
  {
    "objectID": "example.html#setup-analysis",
    "href": "example.html#setup-analysis",
    "title": "Example Analysis",
    "section": "Setup Analysis",
    "text": "Setup Analysis\nAnalysis is conducted in R. We will use the following packages:\n\nlibrary(foreign)\nlibrary(tidyverse)\nlibrary(knitr)\nlibrary(here)\nlibrary(diagis) \n\nThe downloaded data files are placed into a folder called “data” in the working directory. The foreign package allows us to read in the data in a labelled format.\n\nW54 &lt;- foreign::read.spss(here(\"data\",\"ATP W54.sav\"), to.data.frame = TRUE)\nW65 &lt;- foreign::read.spss(here(\"data\",\"ATP W65.sav\"), to.data.frame = TRUE)\n\nWe prepare to merge the two waves of data by renaming the columns that do not have wave number suffixes, and merge the two waves (using participant id “QKEY”) to only include those who responded to both waves.\n\nW54 &lt;- W54 %&gt;% rename_at(vars(!matches(\"*(_W54)\"),-QKEY),\n                         ~paste0(., '_W54'))\nW65 &lt;- W65 %&gt;% rename_at(vars(!matches(\"*(_W65)\"),-QKEY),\n                         ~paste0(., '_W65'))\n\nall &lt;- W54 %&gt;% \n  inner_join(W65,by=\"QKEY\")\n\nThere are 6878 participants who completed Wave 54, 4917 who completed wave 65, and 2747 who completed both waves participants\n\nnrow(W54)\n\n[1] 6878\n\nnrow(W65)\n\n[1] 4917\n\nnrow(all)\n\n[1] 2747\n\n\nNext, we create a composite column for whether a participant experienced household or self job loss (COVIDWORK_a_W65) or income loss (COVIDWORK_b_W65) due to the pandemic, by April 2020.\n\nall &lt;- all %&gt;% mutate(\n  \"COVIDWORK_comp_W65\" = case_when(\n    is.na(COVIDWORK_a_W65) & is.na(COVIDWORK_b_W65) ~ \"Refused/Missing\",\n    COVIDWORK_a_W65 == \"Yes, has happened\" | COVIDWORK_b_W65 == \"Yes, has happened\" ~ \"Yes, has happened\",\n    TRUE ~ \"No, has not happened\"))\n\nn=1081 experienced income or job loss and n=1666 did not.\n\n\n\n    COVIDWORK_comp_W65    n\n1 No, has not happened 1666\n2    Yes, has happened 1081\n\nWe select only the columns of interest for this analysis and rename the Wave 65 financial strain columns to have the same naming schema as Wave 54.\n\nall &lt;- all %&gt;% select(QKEY,WEIGHT_W65, \n                      income_loss=COVIDWORK_comp_W65,\n                      contains(\"WORRY\")) %&gt;% \n  relocate(\"QKEY\",contains(\"RESPONDED\")) \n\nall &lt;- all %&gt;% rename_at(vars(matches(\"(WORRY)(.*)(W65)\")),\n                         ~str_remove(.,\"_\"))\n\nNow, we convert the data into a long format, (where each participant has a separate row for each combination of wave and question). We also convert the worry questions from frequency categories to integers for this exploration, such that a score of 0 for a particular category corresponds to worrying “Never”.\n\nlong &lt;- all %&gt;% pivot_longer(cols = starts_with(\"WORRY\"),\n  names_to = c(\".value\",\"Wave\"),\n  names_pattern = \"(.*_)(W\\\\d+)\") %&gt;% \n  pivot_longer(cols = starts_with(\"WORRY\"),\n  names_to = c(\".value\",\"Q\"),\n  names_pattern = \"(.*2)(.)\") %&gt;% \n  mutate(\"WORRY_int\" = case_when(\n    WORRY2 == \"Never\" ~ 0,\n    WORRY2 == \"Rarely\" ~ 1,\n    WORRY2 == \"Sometimes\" ~ 2,\n    WORRY2 == \"Almost every day\" ~ 3,\n    WORRY2 == \"Every day\" ~ 4,\n    TRUE ~ NA))\n\n\nMissing data\n\nlong %&gt;% \n  group_by(Q,Wave) %&gt;% \n  summarize(\"n_miss\"=sum(is.na(WORRY_int)),\n            \"perc_missing\"=n_miss/n()) %&gt;%\n  knitr::kable()\n\n\n\n\n\nQ\nWave\nn_miss\nperc_missing\n\n\n\n\na\nW54\n18\n0.0065526\n\n\na\nW65\n6\n0.0021842\n\n\nb\nW54\n1038\n0.3778668\n\n\nb\nW65\n1235\n0.4495814\n\n\nc\nW54\n11\n0.0040044\n\n\nc\nW65\n7\n0.0025482\n\n\nd\nW54\n18\n0.0065526\n\n\nd\nW65\n9\n0.0032763\n\n\ne\nW54\n48\n0.0174736\n\n\ne\nW65\n28\n0.0101929\n\n\nf\nW54\n2747\n1.0000000\n\n\nf\nW65\n1233\n0.4488533\n\n\n\n\nWe investigate the missing data patterns for our variables of interest and note that questions b and f have high levels of missingness. Looking at the questionnaire, we can see that b, worry about losing ones job, was only asked to those who were employed. Additionally, 2f (worry about paycuts) was only asked in April 2020 and only to those who were employed, and was not asked in Sept 2019. Therefore, for our initial investigation, we focus on questions a, c, d, e, respectively corresponding to worry about debt, bills, cost of health care, and being able to save enough for retirement, and exclude those with missing data (n=90).\n\nlong &lt;- long %&gt;% filter(!(Q %in% c(\"b\",\"f\")))  %&gt;% \n  mutate(Q = case_when(\n    Q == \"a\" ~ \"Debt\",\n    Q == \"c\" ~ \"Bills\",\n    Q == \"d\" ~ \"Healthcare\",\n    Q == \"e\" ~\"Retirement\"))\n\nany_na &lt;- long %&gt;% \n  group_by(QKEY) %&gt;% \n  summarize(any_na = sum(is.na(WORRY_int))) %&gt;% \n  filter(any_na &gt; 0) %&gt;% pull(QKEY)\n\nlength(any_na)\n\n[1] 90\n\nlong &lt;- long %&gt;% filter(!(QKEY %in% any_na))"
  },
  {
    "objectID": "bash.html",
    "href": "bash.html",
    "title": "bash_part4",
    "section": "",
    "text": "Use wget to download four files that end in .fastq from here. Create a directory to download the data. The top level directory should be called raw_data and there should be a sub-level directory called fastq. The command you write should force the creation of both directories at the same time if either of them do not exist yet. Move all the .fastq files into the fastq sub-level directory. List all the .fastq files that end in a 12 or 13. Search for the string NNNN in the SRR1039512_subset_1.fastq file. In addition to returning the matched line in the .fastq file, your output should also return the two lines before and the four lines after the matching line. Write a for loop in the shell that iterates over each .fastq file. For each .fastq file, do the following. In the first 1000 rows for each file, count the number of lines where the “@” symbol appears. Your final output should be four numbers printed to the screen.\nUse wget to download four files that end in .fastq from here.\nwget https://raw.githubusercontent.com/stephaniehicks/jhustatprogramming2023/main/data/SRR1039508_subset_1.fastq\n \nwget https://raw.githubusercontent.com/stephaniehicks/jhustatprogramming2023/main/data/SRR1039512_subset_1.fastq\n  \nwget https://raw.githubusercontent.com/stephaniehicks/jhustatprogramming2023/main/data/SRR1039509_subset_1.fastq\n   \nwget https://raw.githubusercontent.com/stephaniehicks/jhustatprogramming2023/main/data/SRR1039513_subset_1.fastq\nCreate a directory to download the data. The top level directory should be called raw_data and there should be a sub-level directory called fastq. The command you write should force the creation of both directories at the same time if either of them do not exist yet.\nmkdir -p raw_data/fastq\nMove all the .fastq files into the fastq sub-level directory.\nmv *.fastq raw_data/fastq/.\nList all the .fastq files that end in a 12 or 13.\nls raw_data/fastq/SRR103951*_subset_1.fastq \nline_num=$(grep -n NNNN raw_data/fastq/SRR1039512_subset_1.fastq | cut  -d : -f 1)\necho $line_num\nend_line=$(($line_num + 4))\nhead -n $end_line raw_data/fastq/SRR1039512_subset_1.fastq | tail -n 7\nfor file in raw_data/fastq/*.fastq\ndo \n  head -n 1000 $file | grep \"@\"  | wc -l\ndone"
  },
  {
    "objectID": "example.html#descriptive-analysis",
    "href": "example.html#descriptive-analysis",
    "title": "Example Analysis",
    "section": "Descriptive analysis",
    "text": "Descriptive analysis\nWe will visualize the change in financial worry between September 2019 and April 2020 both in a composite worry score (adding up the sub-scores for debt, bills, health care, and retirement) and for the separate sub-scores, in total and by income or job loss group.\n\n\n\n\n\n\nCaution\n\n\n\nEach wave of ATP data has a column in the downloaded data with associated survey weights (ex. WEIGHT_W65). When using multiple waves of data, it is important to consider which weights will be most appropriate for the sample. In this example analysis, we use Wave 65 weights, as a larger percentage of Wave 65 participants are included in our sample than the percentage of Wave 54 participants. However, in-order to obtain nationally representative estimates, it still may be necessary to create custom weights for the sample used in the analysis.\n\n\n\nFinancial worry between September 2019 and April 2020\nFirst we plot both the un-weighted and the survey weighted mean and standard error of the mean composite worry score, for each wave, along with confidence intervals. We can see that without using survey weights, we would conclude that there are not significant differences in the mean worry score before and after the start of the pandemic, but using survey weighting, there is a significantly difference in worry before and after the start of the pandemic. This highlights the importance of using survey weights.\n\nlong %&gt;% group_by(Wave,QKEY) %&gt;%\n  summarize(worry_sum = sum(WORRY_int)) %&gt;% \n  left_join(select(all,QKEY,WEIGHT_W65),by=\"QKEY\") %&gt;% \n  group_by(Wave) %&gt;% \n  summarize(mean_unweighted=mean(worry_sum),\n            mean_weighted = weighted.mean(worry_sum,WEIGHT_W65),\n            se_unweighted=sd(worry_sum)/sqrt(n()),\n            se_weighted=weighted_se(worry_sum,WEIGHT_W65)) %&gt;%\n  ungroup() %&gt;% \n  pivot_longer(cols = starts_with(\"mean\"),\n  names_to = c(\".value\",\"Weighting_mean\"),\n  names_pattern = \"(.*_)(.*weighted)\") %&gt;% \n  pivot_longer(cols = starts_with(\"se\"),\n  names_to = c(\".value\",\"Weighting_se\"),\n  names_pattern = \"(.*_)(.*weighted)\") %&gt;% \n  filter(Weighting_mean!=Weighting_se) %&gt;%\n  rename(mean=mean_,se=se_) %&gt;% \n  ggplot() +\n  geom_point(aes(colour=Wave, y=mean,x=Weighting_mean)) + \n  geom_errorbar(aes(colour=Wave,ymin=(mean-1.96*se),ymax=(mean + 1.96*se),\n                    x=Weighting_mean),width=.05) +\n  xlab(\"\") + ylab(\"Financial worry\")\n\n\n\n  labs(title=\"Figure 2. Overall change in financial worry score\", \n       subtitle = \"September 2019 and April 2020\",\n       caption=str_wrap(\"Average (95% CI) financial worry score (composite of worries about debt, bills, cost of health care, and being able to save enough for retirement) in Wave 54 (September 2019) and Wave 65 (April 2020) in American Trends Panel data, with and without survey weighting.\"))  +\n  theme_bw()\n\nNULL\n\n\n\n\nBy income or job loss group\nNext we plot weighted average financial worry by income or job loss group. We see that while weighted average worry tends to be higher in the income or job loss group, both groups had a decrease in composite financial worry score.\n\nlong %&gt;% group_by(Wave,income_loss) %&gt;%\n  summarize(mean = weighted.mean(WORRY_int,WEIGHT_W65),\n            se=sd(WORRY_int,na.rm=TRUE)/sqrt(n())) %&gt;% \n  ggplot() +\n  geom_point(aes(x=Wave, y=mean)) + \n  geom_line(aes(x=Wave, y=mean,linetype=income_loss,group=income_loss)) + \n  geom_errorbar(aes(x=Wave,ymin=(mean-1.96*se),ymax=(mean + 1.96*se)),width=.05) +\n  labs(linetype=\"Income or job loss\",\n       title=\"Figure 3. Financial worry score by income or job loss group\",\n       subtitle = \"September 2019 and April 2020\",\n       caption=str_wrap(\"Average (95% CI) financial worry score (composite of worries about debt, bills, cost of health care, and being able to save enough for retirement) in Wave 54 (September 2019) and Wave 65 (April 2020) in American Trends Panel data, with survey weighting. Income or job loss due to teh COVID-19 pandemic was reported in April 2020.\")) +\n  theme_bw()\n\n\n\n\n\n\nAnd by worry type\nFinancially, we disaggregate by worry type. Here we see that worry about bills was higher in April 2020 than in September 2019 for those who experienced income or job loss by April 2020, but that all other categories of worry decreased.\n\nlong %&gt;% group_by(Wave,Q,income_loss) %&gt;%\n  summarize(mean = weighted.mean(WORRY_int,WEIGHT_W65),\n            se=sd(WORRY_int,na.rm=TRUE)/sqrt(n())) %&gt;% \n  ggplot() +\n  geom_point(aes(x=Wave, y=mean,colour=Q)) + \n  geom_line(aes(x=Wave, y=mean,colour=Q,linetype=income_loss,group=interaction(income_loss,Q))) + \n  geom_errorbar(aes(x=Wave,ymin=(mean-1.96*se),ymax=(mean + 1.96*se),colour=Q),width=.05)+ facet_wrap(.~Q)  +\n  scale_colour_discrete(name=\"Worry type\") + \n  labs(linetype=\"Income or job loss\",\n       title=\"Figure 4. Financial worry subscores by income or job loss group\",\n       subtitle = \"September 2019 and April 2020\",\n       caption=str_wrap(\"Average (95% CI) financial worry score by category in Wave 54 (September 2019) and Wave 65 (April 2020) in American Trends Panel data, with survey weighting. Income or job loss due to the COVID-19 pandemic was reported in April 2020.\")) +\n  theme_bw()"
  },
  {
    "objectID": "example.html#conclusions",
    "href": "example.html#conclusions",
    "title": "Example Analysis",
    "section": "Conclusions",
    "text": "Conclusions\nDescriptive plots using survey-weighted means and 95% confidence intervals suggest that financial worries (measured by a composite score made up of four sub-scores) decreased overall between September 2019 and April 2020 (see Figure 2). While this trend is also seen among those who did not report that they experienced income or job loss by April 2020, financial worry score remained more constant among those who did report income or job loss by April 2020 (see Figure 3). When investigating the subscores of financial worries separately (see Figure 4), we see that worry about bills for those who experienced income or job loss increased between September 2019 and April 2020, but decreased for all other types of worries, in both subgroups. This suggests that there may be nuanced of trends by worry type and job loss status and highlight the need for careful data exploration prior to a formal analysis. Future analyses should consider potential confounders and methods such as propensity score weighting to address pre-existing differences between those who experienced income or job loss and those who did not.\n\nData wrangling functions used:\n\nselect\ngroup_by\nsummarize\nmutate\nfilter\npivot_longer\nrename_at\n\n\n\nPlotting functions used:\n\ngeom_bar\ngeom_point\ngeom_errorbar\ngeom_line\nfacet_wrap\n\n\n\nReferences\n\n\n1. The American Trends Panel. https://www.pewresearch.org/our-methods/u-s-surveys/the-american-trends-panel/\n\n\n2. Bartik A, Bertrand M, Lin F, Rothstein J, Unrath M. Measuring the Labor Market at the Onset of the COVID-19 Crisis. National Bureau of Economic Research; 2020:w27613. doi:10.3386/w27613\n\n\n3. Wilkinson LR. Financial Strain and Mental Health Among Older Adults During the Great Recession. The Journals of Gerontology Series B: Psychological Sciences and Social Sciences. 2016;71(4):745-754. doi:10.1093/geronb/gbw001\n\n\n4. Pew Research Center’s American Trends Panel Wave 54 Methodology Report. Pew Research Center; 2019.\n\n\n5. Pew Research Center’s American Trends Panel Wave 65 Methodology Report. Pew Research Center; 2020."
  }
]